30. merge three files. Output should be sorted by id and displayed in id, name, salary, managerName
Solution:
1. create three csv files in local and put in hdfs.
2. 
var manager = sc.textFile("/spark/EmployeeManager.csv")
var managerPairRDD = manager.map(x => (x.split(",")(0), x.split(",")(1)))

var employee = sc.textFile("/spark/EmployeeName.csv")
var employeePairRDD = employee.map(x => (x.split(",")(0), x.split(",")(1)))

var salary = sc.textFile("/spark/EmployeeSalary.csv")
var salaryPairRDD = salary.map(x => (x.split(",")(0), x.split(",")(1)))

var joinedData = employeePairRDD.join(salaryPairRDD).join(managerPairRDD)
//above command returns (E02,((Bhupesh,50000),Satyam))

var sortById = joinedData.sortByKey()

//output should be in id, name, salary, managerName
var finalData = sortById.map(v => (v._1, v._2._1._1, v._2._1._2, v._2._2))

#save output in hdfs as textfile:
finalData.saveAsTextFile("/spark/<new_dir_name>")

31. 
val content = sc.textFile("/spark/test31/content.txt")
val remove = sc.textFile("/spark/test31/remove.txt")
var removeRDD = remove.flatMap(x => x.split(",")).map(word => word.trim)
var contentRDD = content.flatMap(x => x.split(" "))

val bRemove = sc.broadcast(removeRDD.collect().toList)
val filtered = contentRDD.filter(word => !bRemove.value.contains(word))
val pairRDD = filtered.map(word => (word, 1))
val wordCount = pairRDD.reduceByKey(_ + _) //pairRDD.reduceByKey((x, n) => (x + n))
wordCount.saveAsTextFile("/spark/<new_dir_name>")

32. //no space between two filepaths(',')
val content = sc.textfile("/spark/test32/file1.txt,/spark/test32/file2.txt,/spark/test32/file3.txt")
val flatContent = content.flatMap(word => word.split(" "))
val trimmedContent = flatContent.map(word => word.trim)
val removeRDD = sc.parallelize(List("a","the","an","as","a","with","this","these","is","are","in","for","to","and","The","of"))
val filterd = trimmedContent.subtract(removeRDD)
val pairRDD = filterd.map(word => (word, 1))
val wordCount = pairRDD.reduceByKey(_ + _)
//swap key and value
val swapped = wordCount.map(item => item.swap)
//sort by count in reverse order
val sortedOutput = swapped.sortByKey(false)
sortedOutput.saveAsTextFile("/spark/test32/output")
// save compressed output: 
import org.apache.hadoop.io.compress.GzipCodec
sortedOutput.saveAsTextFile("/spark/test32/output", classOf[GzipCodec])
# hdfs dfs -text /spark/test32/output/part*

33. produce (name, salary) and group by salary later
val name = sc.textFile("/spark/test33/EmployeeName.csv")
val salary = sc.textFile("/spark/test33/EmployeeSalary.csv")
//string to int = string.toInt
val nameRDD = name.map(data => (data.split(",")(0).toInt, data.split(",")(1)))
val salaryRDD = salary.map(data => (data.split(",")(0).toInt, data.split(",")(1)))
val joinedData = nameRDD.join(salaryRDD) ==> (E06,(Pavan,45000))

//collect only values
val removeKey = joinedData.values
val swapped = removeKey.map(item => item.swap)
val grpByKey = swapped.groupByKey()

// RDD to collection like Array ==> .collect()
// Array to RDD ==> sc.parallelize(array)

test.map{case(k,v) => k -> sc.makeRDD(v.toSeq)}

34.
val csv = sc.textFile("/spark/test34/user.csv")
val headerAndRows = csv.map(line => line.split(",").map(_.trim)) ==> returns Ljava.lang.String;@15168590
val header = headerAndRows.first
// filter out header:
val data = headerAndRows.filter(x => x(0) != header(0)) //.filter(_(0) != header(0))
// Split to map (header/value pair):
val maps = data.map(splits => header.zip(splits).toMap)
// remove if id = myself:
val result = maps.filter(map => ("id") != "myself")
scala> result.foreach(println)
Map(id -> Rahul, topic -> scala, hits -> 120)

35. sort by name, save back to (id, name), save in a single file:
val name = sc.textFile("/spark/test33/EmployeeName.csv")
val nameRDD = name.map(x => (x.split(",")(0), x.split(",")(1)))
val swapped = nameRDD.map(item => item.swap)
val sortByName = swapped.sortByKey()
val swappedBack = sortByName.map(item => item.swap)
// to save in a single file:
swappedBack.repartition(1).saveAsTextFile("/spark/test34/result")

nameRDD.map(item => item.swap).sortByKey().map(item => item.swap).repartition(1).saveAsTextFile("/spark/test34/result")

36. save it in (id, (all names of same type))
given (type, name)
val name = sc.textFile("/spark/test36/data.csv")
val namePairRDD = name.map(x => (x.split(",")(0), x.split(",")(1)))
val swapped = namePairRDD.map(item => item.swap)

val combinedOutput = namePairRDD.combineByKey(
	List(_),
	(x:List[String], y:String) => y :: x,
	(x: List[String], y:List[String]) => x::y
	)


name.map(item => (item.split(",")(0), item.split(",")(1)).groupByKey().foreach(println)
(1,CompactBuffer(Lokesh, Pavan, Tejas, Kumar, Venkat))
(2,CompactBuffer(Bhupesh, Amit, Ratan, Dinesh, Sheela))

37. save the valid and not valid dates in separate files:
// write all the valid regular expression syntax:
// 11 Jan, 2015 
val reg1 = """(\d+)\s(\w{3})(,)\s(\d{4})""".r
// 6/17/2014
val reg2 = """(\d+)(\/)(\d+)(\/)(\d{4})""".r 
// 22-08-2013
val reg3 = """(\d+)(-)(\d+)(-)(\d{4})""".r 
// Jan 11, 2015
val reg4 = """(\w{3})\s(\d+)(,)\s(\d{4})""".r

val feedbackRDD = sc.textFile("/spark/test37/feedback.txt")
val feedbacksplit = feedbackRDD.map(line => line.split('|'))
val validRecords = feedbacksplit.filter(x => (reg1.pattern.matcher(x(1).trim).matches | reg2.pattern.matcher(x(1).trim).matches | reg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches))
val nonValidRecords = feedbacksplit.filter(x => !(reg1.pattern.matcher(x(1).trim).matches | reg2.pattern.matcher(x(1).trim).matches | reg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches))

// convert each array to String
val valid = validRecords.map(e => (e(0), e(1), e(2)))
val nonValid = nonValidRecords.map(e => (e(0), e(1), e(2)))

valid.repartition(1).saveAsTextFile("/spark/test37/validResult")
nonValid.repartition(1).saveAsTextFile("/spark/test37/nonValidResult")

38. to save RDD as a sequenceFile. code snippet:
import org.apache.hadoop.io.compress.GzipCodec
rdd.map(byetesArray => (A.get(), new B(byetesArray))).saveAsSequenceFile("/hdfsPath")
A = NullWritable
B = byetesArray

39. join two tables: and sum the second columns 
//after joining two files:
(1, ( (9,5), (g, h))) 
(2, ( (7,4), (g, h)))
(3, ( (8,3), (g, h)))
// final output sum: 5 + 4 + 3

1, 9, 5 => 1, (9, 5)
1, g, h => 1, (g, h)

val file1 = sc.textFile("/spark/test39/file1.txt")
val file1RDD = file1.map(x => (x.split(",")(0), (x.split(",")(1), x.split(",")(2).toInt)))

val file2 = sc.textFile("/spark/test39/file2.txt")
val file2RDD = file2.map(x => (x.split(",")(0), (x.split(",")(1), x.split(",")(2))))

val joinedData = file1RDD.join(file2RDD)
val filteredData = joinedData.map(x => x._2._1._2.toInt)
val reduceData = filtered.reduce((x + y) => x + y)
OR val reduceData = filteredData.reduce(_ + _)

40. 
 var mapper = text.map(x => x.split(","))
 mapper.map(x => x.map(x => ({if (x.isEmpty) 0 else x}))).collect()
 mapped.map(x => x.map(x => if (x.isEmpty) 0 else x).collect()

41. 
val au1 = sc.parallelize(List( ("a", Array(1,2), ("b", Array(1,2))))) 
val au2 = sc.parallelize(List( ("a", Array(3), ("b", Array(2)))))

TO generate output like:
Array[(String, Array[Int])] = Array((a, Array(1,2))),("b", Array(1,2)), ("a", Array(3), ("b", Array(2)))

au1.union(au2)

42.
required output:
Dep, des, state, count, cost
sales, Lead, TN, 2, 64000

val rawlines = sc.textFile("/spark/test42/sales.txt")
// create a case class, which can represent its column fields:
case class Employee(dep: String, des: String, cost: Double, state: String)
// split data and create RDD of all Employee object:
val employees =  rawlines.map(_.split(",")).map(row => Employee(row(0), row(1), row(2).toDouble, row(3)))
// create a row as we needed. All group by fields as a key and value as a count for each employee
val keyVals = employees.map(em => ((em.dep, em.des, em.state), (1, em.cost)))
// (a.count + b.count, a.cost + b.cost)
val results = keyVals.reduceByKey{(a, b) => (a._1 + b._1, a._2 + b._2)}

43.
Required output: Array((1, two, 3, 4), (1, two, 5, 6))

val grouped = sc.parallelize(Seq(((1, "two"), List((3,4), (5,6)))))
val flattened = grouped.flatMap{case (key, groupValues) => groupValues.map{value => (key._1, key._2, value._1, value._2)}}

44. count words from each file and make a single file with those file name and highest occuring words:
val file1 = sc.textFile("/spark/test44/text1.txt")
val file2 = sc.textFile("/spark/test44/text2.txt")
val content1 = file1.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _).map(item => item.swap).sortByKey(false).map(e => e.swap)

val content2 = file2.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _).map(item => item.swap).sortByKey(false).map(e => e.swap)

// split the data and create RDD of all 
val file1word = sc.makeRDD(Array(file1.name + " -> " + content1(0)._1 + "-" + content1(0)._2))

45. Two files with tech = first, last, technology amd salary =first, last, salary
Required output: first, last, technology, salary

val map1 = tech.map(item => item.split(",")).map(e => ((e(0), e(1)), e(2)))
val map2 = salary.map(item => item.split(",")).map(e => ((e(0), e(1)), e(2)))
val joined = map1.join(map2)

46. 
val rdd = sc.parallelize(List(("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female", 2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000)))

// convert RDD to pair RDD:
val byKey = rdd.map({case (name, sex, cost) => (name, sex) -> cost})
OR
val byKey = rdd.map({case (name, sex, cost) => ((name, sex), cost)})

val byKeyGrouped = byKey.groupByKey
// sum the cost for each group:
val result = byKeyGrouped.map{case ((name, sex), values) => (name, sex, values.sum)}

47.
val z = sc.parallelize(List(1, 2, 3, 4, 5, 6), 2)
// print out the contents of the RDD with partition labels:
def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = { iter.toList.map(x => "partID:" + index + ", value: " + x + "]").iterator}

z.mapPartitionsWithIndex(myfunc).collect
res28: Array[String] = Array(partID:0, value: 1], partID:0, value: 2], partID:0, value: 3], partID:1, value: 4], partID:1, value: 5], partID:1, value: 6])

z.aggregate(5)(math.max(_,_), _ + _)

48. Python code snippet:
people = []
people.append({'name':'Amit', 'age':45,'gender':'M'})
people.append({'name':'Ganga', 'age':43,'gender':'F'})
people.append({'name':'John', 'age':28,'gender':'M'})
people.append({'name':'Lolita', 'age':33,'gender':'F'})
people.append({'name':'Dont Know', 'age':18,'gender':'T'})

peopleRDD = sc.parallelize(people)
peopleRDD.aggregate((0,0), seqOp, combOp) ==> output 167, 5

seqOP: sum the age of all people as well as count them, in each partition.
combOp: combine results from all partitions.

seqOp = (lambda x, y: (x[0] + y['age'], x[1] + 1))
combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))

49.
val keysWithValuesList = Array("foo=A", "foo=A", "foo=A", "foo=A", "foo=B", "bar=C", "bar=D", "bar=D")
val data = sc.parallelize(keysWithValuesList)

//create key value pairs:
val kv = data.map(_.split("=")).map(v => (v(0), v(1))).cache()

val initialCount = 0;
val countByKey = kv.aggregateByKey(initialCount)(addToCounts, sumPartitionCounts)
define addToCounts and sumPartitionCounts which will produce following results.
countByKey.collect
res3: Array[(String, int)] = Array((foo, 5), (bar, 3))

val addToCounts = (n:Int, v: String) => n + 1
val sumPartitionCounts = (p1: Int, p2: Int) => p1 + p2

import scala.collection._
val initialSet = scala.collection.mutable.HashSet.empty[String]
val addToSet = (s: mutable.HashSet[String], v: String) => s += v
val mergePartitionSets = (p1: mutable.HashSet[String], p2: mutable.HashSet[String]) => p1 ++= p2
val uniqueByKey = kv.aggregateByKey(initialSet)(addToSet, mergePartitionSets)

// Now define two functions (addToSet, mergePartitionSets) such, which will produce following results:
uniqueByKey.collect
res4: Array[(String, scala.collection.mutable.HashSet[String])] = Array((foo, Set(B, A)), (bar, Set(C, D)))

scala> kv.groupByKey().collect
res3: Array[(String, Iterable[String])] = Array((foo,CompactBuffer(A, A, A, A, B)), (bar,CompactBuffer(C, D, D)))

50.
type ScoreCollector = (Int, Double)
type PersonScores = (String, (Int, Double))

val initialScores = Array(("Fred", 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0), ("Wilma", 95.0), ("Wilma", 98.0))
val wilmaAndFredScores = sc.parallelize(initialScores).cache()

//val averagingFunction = (personScore: PersonScores) => {val (name, (numberScores, totalScore)) = personScore(name, totalScore / numberScores)}
// val averageScores = scores.collectAsMap().map(averagingFunction)

val createScoreCombiner = ((score: Double) => (1, score))
val scoreCombiner = ((score2: (Int, Double), score1: Double) => (score2._1 + 1, score1 + score2._2))
val scoreMerger = ((score1: (Int, Double)), (score2: (Int, Double))) => (score1._1 + score2._1, score1._2 + score2._2) 
val scores = wilmaAndFredScores.combineByKey(createScoreCombiner, scoreCombiner, scoreMerger)
scala> scores.collectAsMap().map(x => (x._1, x._2._2 / x._2._1))
res129: scala.collection.Map[String,Double] = Map(Fred -> 91.33333333333333, Wilma -> 95.33333333333333)

scala> scores.collectAsMap().map(x => (x._1, x._2._2 / x._2._1)).foreach(println)
(Fred,91.33333333333333)
(Wilma,95.33333333333333)

51.
val a = sc.parallelize(List(1,2,1,3), 1)
val b = a.map((_, "b"))
val c = a.map((_, "c"))

scala> b.groupByKey().union(c.groupByKey()).groupByKey().collect
res23: Array[(Int, Iterable[Iterable[String]])] = Array((1,CompactBuffer(CompactBuffer(b, b), CompactBuffer(c, c))), (3,CompactBuffer(CompactBuffer(b), CompactBuffer(c))), (2,CompactBuffer(CompactBuffer(b), CompactBuffer(c))))

scala> b.cogroup(c).collect
res24: Array[(Int, (Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b),CompactBuffer(c))), (2,(CompactBuffer(b),CompactBuffer(c))))

"OR"

scala> val x = sc.parallelize(List((1, "apple"), (2, "banana"), (3, "orange"), (4, "kiwi")), 2)
scala> val y = sc.parallelize(List((5, "computer"), (1, "laptop"), (1, "desktop"), (4, "iPad")), 2)
scala> x.cogroup(y).collect
res25: Array[(Int, (Iterable[String], Iterable[String]))] = Array((4,(CompactBuffer(kiwi),CompactBuffer(iPad))), (2,(CompactBuffer(banana),CompactBuffer())), (1,(CompactBuffer(apple),CompactBuffer(laptop, desktop))), (3,(CompactBuffer(orange),CompactBuffer())), (5,(CompactBuffer(),CompactBuffer(computer))))

52.
val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,1,1,1,1,1))
b.countByValue
res41: scala.collection.Map[Int,Long] = Map(5 -> 1, 1 -> 6, 6 -> 1, 2 -> 2, 7 -> 1, 3 -> 1, 8 -> 1, 4 -> 1)


53. 
val a = sc.parallelize(1 to 10, 3)
operation1
b.collect

output1
Array[Int] = Array(2,4,5,8,10)

operation1: 
a.filter(_%2 == 0)
a.filter(x => x%2 == 0).collect

operation2
output2
Array[Int] = Array(1, 2, 3)

operation2:
a.filter(_ < 4).collect
a.filter(x => x < 4).collect

54.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"))
val b = a.map(x => (x.length, x))
b: Array[(Int, String)] = Array((3,dog), (5,tiger), (4,lion), (3,cat), (7,panther), (5,eagle))

b.foldByKey("")(_ + _).collect

OR
b.aggregateByKey("")({case (x, str) => (x + str)}, {case (x1, str1) => (x1 + str1)}).collect
res54: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))

OR
b.aggregateByKey("")((x,y) => (x + y), (x1,y1) => (x1 + y1)).collect

55.
val pairRDD1 = sc.parallelize(List(("cat", 2), ("cat", 5), ("book", 4), ("cat", 12)))
val pairRDD2 = sc.parallelize(List(("cat", 2), ("cup", 5), ("mouse", 4), ("cat", 12)))
scala> pairRDD1.fullOuterJoin(pairRDD2).collect
res137: Array[(String, (Option[Int], Option[Int]))] = Array((cup,(None,Some(5))), (cat,(Some(2),Some(2))), (cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))), (cat,(Some(12),Some(2))), (cat,(Some(12),Some(12))), (book,(Some(4),None)), (mouse,(None,Some(4))))

56.
scala> a.glom().collect // Array separates with partition
res134: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33), 
	Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66), 
	Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))

scala> a.collect()
res135: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)

57.
val a = sc.parallelize(1 to 9, 3)
scala>a.groupBy(x => {if (x % 2 == 0) "even" else "odd"}).collect
res138: Array[(String, Iterable[Int])] = Array((even,CompactBuffer(2, 4, 6, 8)), (odd,CompactBuffer(1, 3, 5, 7, 9)))

58.
val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y).collect
output:
z: Array[Int] = Array(16, 14, 12, 18, 20, 10, 13, 19, 15, 11, 17)

60.
val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val b = a.keyBy(_.length)
b.foreach(println)
(3,dog)
(6,salmon)
(6,salmon)
(3,rat)
(8,elephant)
http://discuss.itversity.com/t/what-is-difference-between-keyby-and-map-operations-in-apache-spark/210/5

scala> val b = a.map(x => (x.length, x)).foreach(println)
(6,salmon)
(6,salmon)
(3,rat)
(3,dog)
(8,elephant)

val c = sc.parallelize(List("dog", "cat", "gnu", "salmon", "rabbit", "turkey", "wolf", "bear", "bee"), 3)
val d = c.keyBy(_.length)
(3,dog)
(6,salmon)
(3,cat)
(3,gnu)
(6,rabbit)
(6,turkey)
(4,wolf)
(4,bear)
(3,bee)

b.join(d).collect // join same keys
res142: Array[(Int, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)), (3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))

d.join(b).collect
res153: Array[(Int, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,salmon)), (6,(rabbit,salmon)), (6,(rabbit,salmon)), (6,(turkey,salmon)), (6,(turkey,salmon)), (3,(dog,dog)), (3,(dog,rat)), (3,(cat,dog)), (3,(cat,rat)), (3,(gnu,dog)), (3,(gnu,rat)), (3,(bee,dog)), (3,(bee,rat)))

61.
same a, b, c, d as in 60.
b.leftOuterJoin(d).collect
res154: Array[(Int, (String, Option[String]))] = Array((6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))), (3,(dog,Some(gnu))), (3,(dog,Some(bee))), (3,(rat,Some(dog))), (3,(rat,Some(cat))), (3,(rat,Some(gnu))), (3,(rat,Some(bee))), (8,(elephant,None)))

scala> d.leftOuterJoin(b).collect
res155: Array[(Int, (String, Option[String]))] = Array((6,(salmon,Some(salmon))), (6,(salmon,Some(salmon))), (6,(rabbit,Some(salmon))), (6,(rabbit,Some(salmon))), (6,(turkey,Some(salmon))), (6,(turkey,Some(salmon))), (3,(dog,Some(dog))), (3,(dog,Some(rat))), (3,(cat,Some(dog))), (3,(cat,Some(rat))), (3,(gnu,Some(dog))), (3,(gnu,Some(rat))), (3,(bee,Some(dog))), (3,(bee,Some(rat))), (4,(wolf,None)), (4,(bear,None)))

62.
scala> b.map(x => (x._1, "x" + x._2 + "x")).collect
res157: Array[(Int, String)] = Array((3,xdogx), (6,xsalmonx), (6,xsalmonx), (3,xratx), (8,xelephantx))
OR
scala> b.mapValues("x" + _ + "x").collect
res158: Array[(Int, String)] = Array((3,xdogx), (6,xsalmonx), (6,xsalmonx), (3,xratx), (8,xelephantx))

63.
scala> val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"))
Four methods: 
scala> b.foldByKey("")(_ + _).collect
res163: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))

scala> b.reduceByKey(_ + _).collect
res164: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))

scala> b.aggregateByKey("")((value, str1) => (value + str1), (str1, str2) => (str1 + str2)).collect
res165: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))

scala> b.combineByKey((value: (String)) => (value), (value1: (String), value2: (String)) => (value1 + value2),
	(value1: (String), value2: (String)) => (value1 + value2)).collect
res166: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))

64.
scala> b.rightOuterJoin(d).foreach(println)
(4,(None,wolf))
(6,(Some(salmon),salmon))
(4,(None,bear))
(6,(Some(salmon),rabbit))
(6,(Some(salmon),turkey))
(6,(Some(salmon),salmon))
(6,(Some(salmon),rabbit))
(6,(Some(salmon),turkey))
(3,(Some(dog),dog))
(3,(Some(dog),cat))
(3,(Some(dog),gnu))
(3,(Some(dog),bee))
(3,(Some(rat),dog))
(3,(Some(rat),cat))
(3,(Some(rat),gnu))
(3,(Some(rat),bee))

65.
val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)
val b = sc.parallelize(1 to a.count.toInt, 2) // count returns Long, so need to convert into Int
val c = a.zip(b).collect
c: Array[(String, Int)] = Array((dog,1), (cat,2), (owl,3), (gnu,4), (ant,5))

c.sortByKey(false).collect // decending order, ascending order sortByKey()
res172: Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2), (ant,5))

66.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)
val b = a.keyBy(_.length)

val c = sc.parallelize(List("ant", "falcon", "squid"), 2)
val d = c.keyBy(_.length)

scala> b.collect
res173: Array[(Int, String)] = Array((3,dog), (5,tiger), (4,lion), (3,cat), (6,spider), (5,eagle))

scala> d.collect
res174: Array[(Int, String)] = Array((3,ant), (6,falcon), (5,squid))

scala> b.subtract(d).collect
res175: Array[(Int, String)] = Array((6,spider), (5,eagle), (3,cat), (3,dog), (5,tiger), (4,lion))

scala> b.subtractByKey(d).collect
res176: Array[(Int, String)] = Array((4,lion))

67.
pyspark // spark python

lines = sc.parallelize(['Its fun to have fun,' , 'but you have to know how.'])
r1 = lines.map(lambda x: x.replace(',' , '').replace('.' , '').replace('-' , '').lower())
r2 = r1.flatMap(lambda x: x.split())
r3 = r2.map(lambda x: (x, 1))
r4 = r3.reduceByKey(lambda x, y: x + y)
r5 = r4.map(lambda x: (x[1], x[0]))
r6 = r5.sortByKey(ascending=False)
r6.take(20) OR r6.collect()

68.
sentences = sc.textFile("/spark/test68/file68.txt") \
.glom() \
.map(lambda x: "".join(x)) \
.flatMap(lambda x: x.split("."))

69. Filter our the word which is less than 2 characters and ignore all empty lines

lines = sc.textFile("/spark/test69/content.txt")
mapped = lines.flatMap(lambda x: x.split(" "))
var filtered = mapped.filter(lambda x: (!x.isEmpty() and x.length > 2)) // scala code
filtered = mapped.filter(lambda x: len(x) > 2) // python code, len(x) != 0

// create an application and store in problem69.py

# Import SparkContext and SparkConf
from pyspark import SparkContext, SparkConf

# Create configuration object and set App name

conf = SparkConf().setAppName("CAA 175 Problem 69")
sc = SparkContext(conf=conf)

# load data from hdfs 
contentRDD = sc.textFile("/spark/test69/content.txt")

mapped = contentRDD.flatMap(lambda x: x.split(" "))
filtered = mapped.filter(lambda x: len(x)> 2)

# save final data
filtered.saveAsTextFile("/spark/test69/result")

// submit this application
spark-submit --master yarn problem69.py

OR,
spark-submit --master local problem69.py

70. word count and save the result on hdfs

from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("CCA175 Problem 70")
sc = SparkContext(conf=conf)

contentRDD = sc.textFile("/spark/test69/content.txt")
mapped = contentRDD.flatMap(lambda x: x.split(" ")) \
.filter(lambda x: len(x) > 0) \
.map(lambda x: (x, 1))

count = mapped.reduceByKey(lambda x, y: x + y) \
.map(lambda x: (x[1], x[0])) \
.sortByKey(False) //capital False, in scala lowercase false

count.saveAsTextFile("/spark/test69/result70")

spark-submit --master yarn problem10.py

71. split each row as (key, value), where key is first word in a line and entire line as value. Filter out empty lines.

lines = sc.textFile("/spark/test71/content71.txt")
mapped = lines.map(lambda x: x.split(" ")) \
.filter(lambda x: len(x) > 0) \  //this counts space as 1.
.map(lambda x: (x[0], x))
[u'Hello', u'this is HadoopExam.com ']

words = lines.filter(lambda x: len(x) > 0) \
.map(lambda x: (x[0], x))
(u'H', u'Hello this is HadoopExam.com ')

//this solution gives right answer
nonempty_lines = lines.filter(lambda x: len(x) > 0)
words = nonempty_lines.map(lambda x : tuple(x.split(' ', 1))) // split(" ", numOfdelimiterToSplit). worked without tuple as well
[u'Hello', u'this is HadoopExam.com ']

>>> words = nonempty_lines.map(lambda x : tuple(x.split(' ', 0))) // split(" ", count)
>>> for i in words.collect(): print(i)
(u'Hello this is HadoopExam.com ',)

>>> words = nonempty_lines.map(lambda x : tuple(x.split(' ', 2)))
>>> for i in words.collect(): print(i)
(u'Hello', u'this', u'is HadoopExam.com ')

// save as sequence file, where key as null and entire line as value.

words.map(lambda line: (None, line)) \
	.saveAsSequenceFile("/spark/test72/output")

// reading back the sequence file data using spark
seqRDD = sc.sequenceFile("/spark/test72/output")

72. given a table names employee2. Spark script to print all the rows and individual column values:
Note: make sure /etc/spark/conf has hive-site.xml file. 
/etc/spark/conf$sudo ln -s /etc/hive/conf/hive-site.xml hive-site.xml //shortcut link
OR				$sudo cp /etc/hive/conf/hive-site.xml /etc/spark/conf/ // copy the file

# Import statements for HiveContext
from pyspark.sql import HiveContext

# create sqlContext
sqlContext = HiveContext(sc)

#Query Hive. It will read default database from hive. To mention other database, "select * from retail.employee2"
employee2 = sqlContext.sql("select * from employee2")

#check if it prints the data:
for row in employee2.collect(): 
print(row)

# print specific column
for row in employee2.collect():
print(row.first_name) 

employee2.collect().show() // shows 20 records.

73. read json data from hdfs and save back to hdfs in json format:
# Import SQLContext
from pyspark import SQLContext

#create instance of SQLContext
sqlContext = SQLContext(sc)

#load json file
employee = sqlContext.jsonFile("/spark/test72/employee.json")
("jsonFile is deprecated. Use read.json() instead.")

#Register RDD as a temp table
employee.registerTempTable("EmployeeTab")

#Select data from employee table
employeeInfo = sqlContext.sql("select * from EmployeeTab")
for row in employeeInfo.collect(): print(row)

# write data in a Text file
employeeInfo.toJSON().saveAsTextFile("/spark/test72/output")

74.
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir /spark/test74/p74_orders \
--m 1

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table order_items \
--target-dir /spark/test74/p74_order_items \
--m 1

orders = sc.textFile("/spark/test74/p74_orders")
order_items = sc.textFile("/spark/test74/p74_order_items")

#Convert RDD into key/value as (order_id as key and rest as value)
ordersKeyValue = orders.map(lambda line: (line.split("," , 1)))
OR
ordersKeyValue = orders.map(lambda line: (int(line.split(",")[0]), line))
orderItemsKeyValue = order_items.map(lambda line: (int(line.split(",")[0]), line))

joinedData = ordersKeyValue.join(orderItemsKeyValue)
(4, (u'4,2013-07-25 00:00:00.0,8827,CLOSED', u'4,2,403,1,129.99,129.99'))
(8, (u'8,2013-07-25 00:00:00.0,2911,PROCESSING', u'8,4,1014,4,199.92,49.98'))

#fetch selected columns from joined data: OrderId, Order date, amount collected
revenuePerOrder = joinedData.map(lambda row: (row[0], row[1][0].split(",")[1], float(row[1][1].split(",")[4])))
>>> for i in revenuePerOrder.take(2): print(i)
... 
(4, u'2013-07-25 00:00:00.0', 129.99000000000001)
(8, u'2013-07-25 00:00:00.0', 199.91999999999999)

#calculate total order placed for each date, and produced the output sorted by date:
# distinct(date, order_id)
distinctOrdersDate1 = joinedData.map(lambda row: (row[1][0].split(",")[1], str(row[0]))).distinct()
distinctOrdersDate1.map(lambda key: (key[0], 1)).reduceByKey(lambda x, y: x + y).sortByKey().count()
OR
distinctOrdersDate = joinedData.map(lambda row: (row[1][0].split(",")[1] + " , " +  str(row[0]))).distinct()
>>> for i in distinctOrdersDate.take(5): print(i)
... 
2014-04-26 00:00:00.0 , 44349 
2014-03-11 00:00:00.0 , 37225

totalOrderPerDay = distinctOrdersDate.map(lambda row: (line.split(",")[0], 1)).reduceByKey(lambda x, y: x + y).sortByKey()

75. With above data calculate entire revenue, maximum, minimum, and average
extractRevenue = joinedData.map(lambda x : float(x[1][1].split(",")[4])
entireRevenue = extractRevenue.reduce(lambda x, y: x + y)
maximum = extractRevenue.reduce(lambda x, y: x if (x > y) else y)
minimum = extractRevenue.reduce(lambda x, y: x if (x < y) else y)
average = entireRevenue / extractRevenue.count()

76. using pyspark calculate the number of order for each status using countByKey(), groupByKey(), reduceByKey(). aggregateByKey(), combineByKey().
columns of order table: (order_id, order_date, order_customer_id, order_status)

order = sc.textFile("/spark/test74/p74_orders")

#countByKey:
keyValue1 = order.map(lambda line: (line.split(",")[3], ""))
output = keyValue1.countByKey().items()

#groupByKey:
keyValue2 = order.map(lambda line: (line.split(",")[3], 1))
output1 = keyValue2.groupByKey().map(lambda kv: (kv[0], sum(kv[1])))

#grpupByKey() creates: 
(u'SUSPECTED_FRAUD', <pyspark.resultiterable.ResultIterable object at 0x2b26210>)
//(u'CLOSED', <pyspark.resultiterable.ResultIterable object at 0x41d20d0>) 

#then later map creates:
(u'PENDING', 7610)
(u'SUSPECTED_FRAUD', 1558)

#reduceByKey:
output2 = keyValue2.reduceByKey(lambda x, y : x + y)

#aggregateByKey:
keyValue3 = order.map(lambda line: (line.split(",")[3], line))
output3 = keyValue3.aggregateByKey(0, lambda a, b: a + 1, lambda a, b: a + b)

#combineByKey:
output4 = keyValue3.combineByKey(lambda value: 1, lambda acc, value: acc + 1, lambda acc1, acc2: acc1 + acc2)

77. 
A. calculate total revenue per day and per order. calculate total and average revenue for each date using combineByKey and aggregateByKey.
orders = sc.textFile("/path in hdfs")
order_items = sc.textFile("/path in hdfs")

ordersKeyValue = orders.map(lambda x: (int(x.split(",")[0]), x))
orderItemsKeyValue = order_items.map(lambda x: (int(x.split(",")[1]), x))
joinedData = ordersKeyValue.join(orderItemsKeyValue)
totalRevenue = joinedData.map(lambda x: ((x[0], x[1][0].split(",")[1]), float(x[1][1].split(",")[4])))
totalRevenuePerDayPerOrder = totalRevenue.reduceByKey(lambda x, y: x + y)
dateAndRevenueTuple = totalRevenuePerDayPerOrder.map(lambda x: (x[0][1], x[1]))
(u'2013-07-25 00:00:00.0', 199.99000000000001)
(u'2013-07-25 00:00:00.0', 199.99000000000001)

B. calculate total and average revenue for each date using combineByKey and aggregateByKey. So the output as (Date, total revenue per date, total number of dates)
#combineByKey:
totalRevenueAndCountByDate = dateAndRevenueTuple.combineByKey(lambda initialVal: (initialVal, 1), \
	lambda revenue, totalRevenue: (revenue[0] + totalRevenue, revenue[1] + 1), \
	lambda revenue, totalRevenue: (round(revenue[0] + totalRevenue[0], 2), revenue[1] + totalRevenue[1]))
(u'2013-09-19 00:00:00.0', (39729.159999999982, 206))
(u'2013-12-06 00:00:00.0', (50532.809999999961, 256))
// to calculate average revenue per each date:
averageRevenuePerDate = totalRevenueAndCountByDate.map(lambda x: \
	(x[0], x[1][0] / x[1][1]))

#aggregateByKey:
totalRevenueAndCountDate = dateAndRevenueTuple.aggregateByKey((0, 0), \
	lambda initialVal, value: (initialVal[0] + value,  initialVal[1] + 1), \
	lambda value1, value2: (value1[0] + value2[0], value1[1] + value2[1]))

78.
A. calculate total revenue per day and per customer

totalRevenuePerDayPerCustomer = joinedData.map(lambda x: ((x[1][0].split(",")[1], x[1][0].split(",")[2]), float(x[1][1].split(",")[4])).reduceByKey(lambda x, y: x + y)
maximumRevenue = totalRevenuePerDayPerCustomer.map(lambda x: x[1]).reduce(lambda x, y: (x if (x > y) else y))

B. calculate maximum revenue customer
maximumRevenueCustomer = totalRevenuePerDayPerCustomer.map(lambda x: (x[0][1], x[1])).filter(lambda x: x[1] == maximumRevenue)

79. use top(), takeOrdered(), sortByKey() to do data ordering or ranking and fetch top 10 elements
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products \
--target-dir /spark/test79/products
--m 1

productRDD = sc.textFile("/spark/test79/products")

#filter out all the empty prices:
nonempty_lines = productRDD.filter(lambda x: len(x.split("|")[4]) > 0)
#ascending:
sortedPriceProducts = nonempty_lines.map(lambda line: (float(line.split("|")[4]), line.split("|")[2])).sortByKey() 

#descending:
sortedPriceProducts = nonempty_lines.map(lambda line: (float(line.split("|")[4]), line.split("|")[2])).sortByKey(False)

# sort in descending order by top() function:
sortedPriceProducts1 = nonempty_lines.map(lambda line: (float(line.split("|")[4]), line.split("|")[2])).top(10)
>>> for i in sortedPriceProducts1: print(i) // no need any function to print like take, collect. 
... 
(1999.99, u'SOLE E35 Elliptical')
(1799.99, u'SOLE F85 Treadmill')

#highest price products name:
highestProducts = nonempty_lines.map(lambda line: (float(line.split("|")[4]), line.split('|')[2])).sortByKey(False).take(1)

#sort data based on product_price as ascending and product_id in ascending order, using takeOrdered() function:
#((price, id), name)
sortedPriceProducts2 = nonempty_lines.map(lambda line: ((float(line.split("|")[4]), \
int(line.split("|")[0])), \
line.split("|")[2])) \
.takeOrdered(10, lambda x: (x[0][0], x[0][1]))

#ascending:
>>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
[1, 2, 3, 4, 5, 6]
#descending
>>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
[10, 9, 7, 6, 5, 4]

#sort data based on product_price as descending and product_id in ascending order, using takeOrdered() function:
# minus(-) parameter used for descending ordering, only for numeric value
sortedPriceProducts3 = nonempty_lines.map(lambda line: ((float(line.split("|")[4]), \
int(line.split("|")[0])), \
line.split("|")[2])) \
.takeOrdered(10, lambda x: (-x[0][0], x[0][1]))

80. sort the products data sorted by product price per category, use product_category_id column to group by category:
#create data set like (categoryId, (id, name, price))

mappedRDD = nonempty_lines.map(lambda line: (line.split("|")[1], (line.split("|")[0], line.split("|")[2], float(line.split("|")[4]))))

groupByCategoryId = mappedRDD.groupByKey()

#sort the data in each category based on price in ascending order using sorted() function.
#sorted function is to sort an iterable, key = on which we want to sort (price)
groupByCategoryId.map(lambda x: sorted(x[1], key=lambda y: y[2])).take(5)

#sort the data in each category based on price in descending order using sorted() function. 
groupByCategoryId.map(lambda x: sorted(x[1], key=lambda y: y[2], reverse=True)).take(5)

#sortBy function:
b = sc.parallelize([('t', 3),('b', 4),('c', 1)])
bSorted = b.sortBy(lambda a: a[1])
bSorted.collect()
...
[('c', 1),('t', 3),('b', 4)]

81. create ORC table using SparkSql. Load data in Hive table, create a hive paraquet table using SparkSql and load data in it.
val products = sc.textFile("/spark/test81/product.csv")

# define the schema using a case class:
case class Product(productId: Integer, code: String, name: String, quantity: Integer, price: Float)

val prodRDD = products.map(_.split(",")).map(p => Product(p(0).toInt, p(1), p(2), p(3).toInt, p(4).toFloat))

#create data frame:
val prdDF = prodRDD.toDF()

# store data in hive warehouse directory. (this will not create table)
import org.apache.spark.sql.SaveMode
prodDF.write.mode(SaveMode.Overwrite).format("orc").saveAsTable("product_orc_table")

#create table in hive using data stored in warehouse:
create external table products (productId int, code String, name String, quantity int, price float) stored as ORC Location "/user/hive/warehouse/product_orc_table";

#create a parquet table in scala:
import org.apache.spark.sql.SaveMode
prodDF.write.mode(SaveMode.Overwrite).format("parquet").saveAsTable("/spark/test81/product_parquet_table")

#In Hive
create external table product_parquet (productId int, code String, name String, quantity int, price float) stored as paraquet location "/user/hive/warehouse/product_parquet_table";
select * from product_parquet;

82. Given table in hive. Using SparkSQL accomplish queries:
# make sure copying hive-site.xml in /usr/lib/spark/conf/
sudo su root
cp /usr/lib/hive/conf/hive-site.xml /usr/lib/spark/conf/

import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)

a. select all the products name and quantity having quantity <=2000: if database is not mentioned it will take from default:
val results = sqlContext.sql("select name, quantity from retail.products where quantity <= 2000 ")
results.show()

b. select name and price of the product having code as 'PEN'
val results = sqlContext.sql("select name, price from retail.products where code = 'PEN'") 

c. select all the products, which name starts with PENCIL:
sqlContext.sql("select * from retail.products where upper(name) like 'PENCIL%' ").show()

d. select all products which "name" begins with 'P', followed by any two characters, followed by space, followed by zero or more characters:
sqlContext.sql("select * from retail.products where name like 'P__%' ")

83. 
a. select all the records with quantity >= 5000 and name starts with 'Pen'
import org.apache.spark.sql.hive.HiveContext
sqlContext = new HiveContext(sc)
sqlContext.sql("select * from retail.products where quantity >= 5000 and name like 'Pen%' ")

b. select all the records with quantity >= 5000, price is less than 1.24 and name starts with 'Pen'
sqlContext.sql("select * from retail.products where quantity >= 5000 and price < 1.24 and name like 'Pen%' ")

c. select all the records which does not have quantity >= 5000 and name does not starts with 'Pen'
sqlContext.sql("select * from retail.products where NOT (quantity >= 5000 and name like 'Pen%') ")

d. select all the records which name is 'Pen Red', 'Pen Black'
sqlContext.sql("select * from retail.products where name like 'Pen Red' or name like 'Pen Black' ")
or,
sqlContext.sql("select * from retail.products where name IN('Pen Red', 'Pen Black') ")

e. select all the products which has price Between 1.0 and 2.0 and quantity between 1000 and 2000
sqlContext.sql("select * from retail.products where (price between 1.0 and 2.0) and (quantity between 1000 and 2000)")

84.
a. select all the products which has product code as null
sqlContext.sql("select * from retail.products where code IS null ")

b. select all the products, whose name stares with Pen and results should be order by Price descending order.
sqlContext.sql("select * from retail.products where name like 'Pen%' order by price desc ")

c. select all the products, whose name starts with Pen and results should be order by Price descending order and quantity ascending order.
sqlContext.sql("select * from retail.products where name like 'Pen%' order by price desc, quantity")

d. select top 2 products by price
sqlContext.sql("select * from retail.products order by price desc limit 2")

85.
a. select all the columns from products table with output header:
#tilda(`) in "unit price"
val results = sqlContext.sql("select productId as ID, code as Code, name as Description, price as `Unit Price` from retail.products")

b. select code and name both separated by '-' and header name should be `Product Description`
sqlContext.sql("select concat(code, '-', name) as `Product Description`, price from retail.products ")

c. select all distinct prices
sqlContext.sql("select distinct(price) from retail.products ")

d. select distinct price and name combination:
sqlContext.sql("select distinct price, name from retail.products ")

e. select all price data sorted by both code and productID combination.
sqlContext.sql("select price from retail.products order by code, productId")

f. count number of products:
sqlContext.sql("select count(*) as count from retail.products")

g. count number of products for each code:
sqlContext.sql("select count(code) from retail.products group by code")

sqlContext.sql("select count(*) as count from retail.products group by code order by count desc")

86.
a. select maximum, minimum, average, standard deviation, and total quantity:
sqlContext.sql("select max(price) as maximum, min(price) as minimum, avg(price) as average, std(price) as `standard deviation`, sum(price) as `total quantity` from retail.products")

b. select minimum and maximum price for each product code:
sqlContext.sql("select max(price), min(price) from retail.products group by code")

c. select maximum, minimum, average, standard deviation, and total quantity for each product code, however make sure average and standard deviation will have maximum two decimal values:
sqlContext.sql("select max(price), min(price), avg(price), round(std(price), 2), sum(price) from retail.products group by code ")
OR
sqlContext.sql("select max(price), min(price), cast(avg(price) as decimal(7,2)) as average, cast(std(price) as decimal(7,2)) as std, sum(price) from retail.products group by code ")

d. select all the product code and average price only where product count is more than or equal to 3:
sqlContext.sql("select code, count(*) as count , avg(price) from retail.products group by code having count >= 3")

e. select maximum, minimum, average and total of all the products for each code. Also produce the same across all the products
sqlContext.sql("select max(price), min(price), cast(avg(price) as decimal(7,2)) as average from retail.products group by code with rollup")

|    _c0| _c1|average|
+-------+----+-------+
|9999.99|0.48|1667.45| // average of all price
|9999.99|0.48|3333.65|
|   1.25|1.23|   1.24|
+-------+----+-------+

87. 
val product = sc.textFile("/spark/test87/product.csv")
val supplier = sc.textFile("/spark/test87/supplier.csv")
val product_supplier = sc.textFile("/spark/test87/product_supplier.csv")

case class Product(productId: Int, productCode: String, name: String, quantity: Int, price: Float, supplierId: Int)
case class Supplier(supplierId: Int, name: String, phone: String)

val productRDD = product.map(x => x.split(",")).map(p => Product(p(0).toInt, p(1), p(2), p(3).toInt, p(4).toFloat, p(5).toInt))
val supplierRDD = supplier.map(_.split(",")).map(s => Supplier(s(0).toInt, s(1), s(2)))

#this is used to implicitly convert a RDD to a DataFrame:
import sqlContext.implicits._

#Import Spark SQL data types and Row
import org.apache.spark.sql._
val sqlContext = new SQLContext(sc)

val prodDF = productRDD.toDF()
val suppDF = supplierRDD.toDF()

prodDF.registerTempTable("products")
suppDF.registerTempTable("suppliers")

val results = sqlContext.sql("select products.name, price, suppliers.name from products join suppliers on products.supplierId = suppliers.supplierId where price < 0.6")

88.
a. same product can be supplied by multiple supplier. Find each product, its price according to each supplier.
select products.name, price, suppliers.name from product_supplier join suppliers on product_supplierId = suppliers.supplierId join products on product_supplier.productId = productsId

b. Find all the supplier name, who are supplying 'Pencil 3B'
select suppliers.name from products join suppliers on products.supplierId = suppliers.supplierId where productCode = 'Pencil 3B'

select distinct suppliers.name from products join suppliers on products.supplierId = suppliers.supplierId join product_supplier on products.productId = product_supplier.productId where products.name = 'Pencil 3B'

c. Find all the products, which are supplied by ABC Traders:
select distinct products.name from products join suppliers on products.supplierId = suppliers.supplierId join product_supplier on suppliers.supplierId = product_supplier.supplierId where suppliers.name = 'ABC Traders'

89. Given patient data:
import org.apache.spark.sql._
val sqlContext = new SQLContext(sc)

import sqlContext.implicits._ // to convert RDD to DF implicitly:

val patient = sc.textFile("/spark/test87/patient.csv")
case class Patient(patientId: Integer, name: String, dob: String, lastDov: String)
val patRDD = patient.map(_.split(",")).map(p => Patient(p(0).toInt, p(1), p(2), p(3)))
patRDD.toDF().registerTempTable("patients")

a. Find all the patients whose lastVisitDate between current time and '2012-09-15'
select * from patients where TO_Date(cast(UNIX_TIMESTAMP(lastDov, 'yyyy-MM-dd') as TIMESTAMP)) between '2012-09-15' and current_timestamp() order by lastDov
OR
select * from patients where from_unixtime(UNIX_TIMESTAMP(lastDov, 'yyyy-MM-dd')) between '2012-09-15' and current_timestamp() order by lastDov

b. Find all the patients who born in 2011
select * from patients where year(to_date(cast(UNIX_TIMESTAMP(dob, 'yyyy-MM-dd') as TIMESTAMP))) = '2011'

c. Find all the patient age
select  datediff(current_timestamp(), to_date(cast(UNIX_TIMESTAMP(dob, 'yyyy-MM-dd') as TIMESTAMP))) / 365 as age from patients

d. List patients whose last visited more than 60 days ago
select * from patients where datediff(current_timestamp(), to_date(cast(UNIX_TIMESTAMP(lastDov, 'yyyy-MM-dd') as TIMESTAMP))) > 60 

e. select patients 18 years old or younger
select * from patients where to_date(cast(UNIX_TIMESTAMP(dob, 'yyyy-MM-dd') as TIMESTAMP)) > date_sub(current_date(), 18 *365);

10/11/12

'yy/MM/dd'

90.
course.txt
id,course
1,Hadoop
2,Spark
3,HBase

fee.txt
id,fee
2,3900
3,4200
4,2900

a. select all the courses and their fees, whether fee is listed or not
select c.* , f.* from courses c left join fees f on c.id = f.id 

b. select all the available fees and respective course. If course does not exists still list the fee.
select c.*, f.* from courses c right join fees f on c.id = f.id

c. select all the courses and their fees, whether fee is listed or not. However, ignore records having fee as null
select c.*, f.* from courses c left join fee f where c.id = f.id where f.id IS NULL 
OR select * from from courses c join fee f where c.id = f.id

id, course, id, fee
1,	hadoop,	null, null
2,  spark,	2, 3900
3,	hbase,	3	4200

91.
#read json file from hdfs:
val employee = sqlContext.read.json("/spark/test91/employee.json")

#store data in hdfs in another format
employee.write.parquet("/spark/test91/employee.parquet")

val parq_data = sqlContext.read.parquet("/spark/test91/employeeDir") 

parq_data.registerTempTable("employees")
val emp = sqlContext.sql("select * from employee")

import org.apache.spark.sql.SaveMode
sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")
employee.write.mode(SaveMode.Overwrite).parquet("/spark/test91/employeeParquet") // parquet file with snappy compression

92. application class name com.hadoopexam.MyTask  and run in cluster node.
spark-submit XXX --master yarn \
YYY $SPARK_HOME/lib/hadoopexam.jar 10

XXX: --class com.hadoopexam.MyTask 
YYY: --deploy-mode cluster

93.run application with locally 8 thread or locally on 8 cores.
spark-submit --class com.hadoopexam.MyTask XXX \
--deploy-mode cluster $SPARK_HOME/lib/hadoopexam.jar 10

XXX: --master local[8] 

94. run spark app on yarn with each executor 20GB and number of executors should be 50.
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
--class com.hadoopexam.MyTask \
XXX \
--deploye-mode cluster\ #can be client for client mode
YYY \
ZZZ \
/path/to/hadoopexam.jar \
1000

XXX: --master yarn
YYY: --executor-memory 20GB
ZZZ: --num-executors 50

95. run spark app on yarn with each executor maximum heap size to be 512MB and number of processor cores to allocate on each executor will be 1. application required 3 values as arguments V1, V2, V3.
.bin/spark-submit --class com.hadoopexam.MyTask --master yarn-cluster --num-executors 3 --driver-memory 512m XXX YYY lib/hadoopexam.jar ZZZ

XXX: --executor-memory 512m
YYY: --executor-core 1
ZZZ: V1 V2 V3

96. spark application required extra Java option:
-XX: